def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer
    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return dA_prev:  Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    :return dW: Gradient of the cost with respect to W (current layer l), same shape as W
    :return db: Gradient of the cost with respect to b (current layer l), same shape as b
    """
    pass


def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer.
    The function first computes dZ and then applies the linear_backward function.
    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation:
    :return dA_prev: Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    :return dW: Gradient of the cost with respect to W (current layer l), same shape as W
    :return db: Gradient of the cost with respect to b (current layer l), same shape as b
    """
    pass


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return dZ: gradient of the cost with respect to Z
    """
    pass


def softmax_backward(dA, activation_cache):
    """
    Implements backward propagation for a softmax unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return dZ: gradient of the cost with respect to Z
    """
    pass


def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation process for the entire network.
    :param AL:  the probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: the true labels vector (the "ground truth" - true classifications)
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
    :return Grads: a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...

    """
    pass


def update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param grads: a python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: the learning rate used to update the parameters (the “alpha”)
    :return parameters: the updated values of the parameters object provided as input
    """
    pass
