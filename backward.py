import numpy as np


def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer
    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return dA_prev:  Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    :return dW: Gradient of the cost with respect to W (current layer l), same shape as W
    :return db: Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev = cache['A_prev']
    A_prev_t = A_prev.transpose()
    W = cache['W']
    W_t = W.transpose()
    m = len(dZ[0])

    dA_prev = (1 / m) * np.matmul(W_t, dZ)  # TODO: not sure 100% about this
    dW = (1 / m) * np.matmul(dZ, A_prev_t)
    db = (1 / m) * np.sum(dZ, axis=1)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer.
    The function first computes dZ and then applies the linear_backward function.
    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache (A,W,b,Z)
    :param activation:
    :return dA_prev: Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    :return dW: Gradient of the cost with respect to W (current layer l), same shape as W
    :return db: Gradient of the cost with respect to b (current layer l), same shape as b
    """
    if activation == 'softmax':
        dZ = softmax_backward(dA, cache)
    else:  # activation = 'relu'
        dZ = relu_backward(dA, cache)

    dA_prev, dW, db = Linear_backward(dZ, cache)

    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return dZ: gradient of the cost with respect to Z
    """
    Z = activation_cache['Z']
    da_dz = np.vectorize(lambda zi: 0 if zi <= 0 else 1)(Z)
    dZ = dA * da_dz
    return dZ


def softmax_backward(dA, activation_cache):
    """
    Implements backward propagation for a softmax unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation) ->>> Gilad said on forum that it ok to assume we get here A_L (soft_max results) and True_labels
    :return dZ: gradient of the cost with respect to Z
    """
    a_L = activation_cache['AL']  # our's softmax (last layer) probabilities
    t_L = activation_cache['TL']  # True labels
    dZ = dA * (a_L - t_L)  # (a_L - t_L) is da_dz  TODO: not sure, need to verify
    return dZ


def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation process for the entire network.
    :param AL:  the probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: the true labels vector (the "ground truth" - true classifications)
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
    :return Grads: a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...

    """
    num_layers = len(caches)
    grads = {}
    caches[num_layers - 1]['TL'] = Y  # true labels are part of the cache of last layer
    caches[num_layers - 1]['AL'] = AL
    dA_prev = None
    for layer_i in range(num_layers, 0, -1):
        activation = 'softmax' if layer_i == num_layers else 'relu'
        dA = dA_prev if layer_i < num_layers else -(Y / AL) + (1 - Y) / (1 - AL)
        dA_prev, dW, db = linear_activation_backward(dA, caches[layer_i - 1], activation)
        grads.update({f'dA{layer_i}': dA, f'dW{layer_i}': dW, f'db{layer_i}': db})

    return grads


def update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param grads: a python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: the learning rate used to update the parameters (the “alpha”)
    :return parameters: the updated values of the parameters object provided as input
    """
    n_layers = int(len(parameters) / 2)
    for i in range(1, n_layers + 1):
        parameters[f'W{i}'] -= learning_rate * grads[f'dW{i}']
        parameters[f'b{i}'] -= learning_rate * grads[f'db{i}']
    return parameters
