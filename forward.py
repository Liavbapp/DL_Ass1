import numpy as np


def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network
    :return: a dictionary containing the initialized W and b parameters of each layer
    """
    params = {f'W{i + 1}': np.random.rand(layer_dims[i + 1], layer_dims[i]) for i in range(len(layer_dims) - 1)}
    params.update({f'b{i + 1}': np.zeros(layer_dims[i + 1]) for i in range(len(layer_dims) - 1)})

    return params


def linear_forward(A, W, b):
    """
    linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return Z: the linear component of the activation function
    :return linear_cache: a dictionary containing A, W, b
    """
    # A_t = np.expand_dims(A, axis=1)
    b_t = np.expand_dims(b, axis=1)
    Z = np.matmul(W, A) + b_t
    linear_cache = {'A': A, 'W': W, 'b': b}
    return Z, linear_cache


def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return A: the activations of the layer
    :return activation_cache: returns Z, which will be useful for the backpropagation
    """
    exp = np.exp(Z)
    A = exp / np.sum(exp)
    return A, {'Z': Z}


def safe_softmax(Z):
    Z_safe = Z - np.max(Z)
    return softmax(Z_safe)[0], {'Z': Z}


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return A: the activations of the layer
    :return activation_cach: returns Z, which will be useful for the backpropagation
    """

    def relu_func(zi):
        return zi if zi > 0 else 0

    relu_func = np.vectorize(relu_func)
    A = np.array([relu_func(z) for z in [Z[:, i] for i in range(0, len(Z[0]))]]).transpose()
    return A, {'Z': Z}


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string,     either “softmax” or “relu”)
    :return A: the activations of the current layer
    :return cache: a joint dictionary containing both linear_cache and activation_cache
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    A_cur, activation_cache = relu(Z) if activation == 'relu' else safe_softmax(Z)

    return A_cur, {**linear_cache, **activation_cache}


def L_model_forward(X, parameters, use_batchnorm):  # Todo: add testing to check it
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
    :return AL: the last post-activation value
    :return caches: a list of all the cache objects generated by the linear_forward function
    """
    caches = []
    num_layers = int(len(parameters) / 2)
    activations_prev = X
    A_i = None
    for i in range(1, num_layers + 1):
        W_i = parameters[f'W{i}']
        b_i = parameters[f'b{i}']
        activation_function = 'softmax' if i == num_layers else 'relu'
        A_i, lin_cache_i = linear_activation_forward(activations_prev, W_i, b_i, activation_function)
        lin_cache_i.update({'A_prev': X}) if i == 1 else lin_cache_i.update({'A_prev': caches[i-2]['A']})
        A_i = apply_batchnorm(A_i) if use_batchnorm else A_i
        activations_prev = A_i
        caches.append(lin_cache_i)

    return A_i, caches


def compute_cost(AL, Y):
    """
    the cost function defined by equation
    :param AL:  probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return cost: the cross-entropy cost
    """
    m = len(AL[0])
    cost = (-1 / m) * sum(np.matmul(Y[:, i], np.log(AL[:, i])) for i in range(0, m))
    return cost
    # for i in range(0, m):  # TODO: check if can vectorize
    #     labels_t = Y[:, i]
    #     softmax_pred = np.log(AL[:, i])
    #     cost += np.matmul(labels_t, softmax_pred)
    # cost = (-1 / m) * cost
    # return cost


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return NA: the normalized activation values, based on the formula learned in class
    """
    epsilon = 0.000000001  # prevent divding by zero
    mu = np.average(A)
    variance = np.var(A)
    A_norm = (A - mu) / np.sqrt(variance) if variance > 0 else (A - mu) / np.sqrt(variance + epsilon)

    return A_norm
