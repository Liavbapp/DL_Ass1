

def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network
    :return: a dictionary containing the initialized W and b parameters of each layer
    """
    pass


def linear_forward(A, W, b):
    """
    linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return Z: the linear component of the activation function
    :return linear_cache: a dictionary containing A, W, b
    """
    pass

def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return A: the activations of the layer
    :return activation_cache: returns Z, which will be useful for the backpropagation
    """
    pass


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return A: the activations of the layer
    :return activation_cach: returns Z, which will be useful for the backpropagation
    """
    pass

def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return A: the activations of the current layer
    :return cache: a joint dictionary containing both linear_cache and activation_cache
    """
    pass

def L_model_forward(X, parameters, use_batchnorm):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
    :return AL: the last post-activation value
    :return caches: a list of all the cache objects generated by the linear_forward function
    """
    pass

def compute_cost(AL, Y):
    """
    the cost function defined by equation
    :param AL:  probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return cost: the cross-entropy cost
    """
    pass

def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return NA: the normalized activation values, based on the formula learned in class
    """
    pass
